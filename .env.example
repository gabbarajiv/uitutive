# Environment Configuration Example
# Copy this file to .env and update values as needed

# ====================================
# Local Development Environment
# ====================================
# Ollama local API configuration
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=mistral
OLLAMA_TEMPERATURE=0.7
OLLAMA_TOP_K=40
OLLAMA_TOP_P=0.9

# API timeout in milliseconds
API_TIMEOUT=30000

# ====================================
# Available Ollama Models (examples)
# ====================================
# mistral        - Fast, good quality responses
# llama2         - Meta's Llama 2 model
# neural-chat    - Intel's Neural Chat
# orca-mini      - Smaller, faster model
# dolphin-mixtral - Dolphin optimized version
# openchat       - Open-source alternative
#
# To download a model:
# ollama pull mistral
# ollama pull llama2
# ollama pull neural-chat

# ====================================
# Local Ollama Setup Instructions
# ====================================
# 1. Download Ollama from https://ollama.ai
# 2. Install and start Ollama
# 3. Pull a model: ollama pull mistral
# 4. Ollama will be available at http://localhost:11434
# 5. Update OLLAMA_MODEL in this file as needed

# ====================================
# Production Environment (for CI/CD)
# ====================================
# NODE_ENV=production
# OLLAMA_API_URL=https://your-ollama-server.com:11434
# OLLAMA_MODEL=mistral
